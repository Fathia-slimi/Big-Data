{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathia-slimi/Big-Data/blob/main/bigdataproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_JGWfXRdeqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089dcdff-f3f6-4990-ea51-f6b6b9f1300c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (2.7.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.2.1)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (from scrapy) (3.4.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.7.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.6)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.7/dist-packages (from scrapy) (38.0.3)\n",
            "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.0.6)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.1.0)\n",
            "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.7.0)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (5.5.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        " !pip install scrapy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "2qRjgpz8iPVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "4n1dtu2jiP9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONzFADqpeTdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae1f02c-add4-4c25-ff41-502048523474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting QuotesSpider.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile QuotesSpider.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = 'quotes'\n",
        "    start_urls = [\n",
        "        ''https://quotes.toscrape.com/page/1/',\n",
        "        'https://quotes.toscrape.com/page/2/',\n",
        "    ]\n",
        "\n",
        "    def parse(self, response):\n",
        "        for quote in response.css('div.quote'):\n",
        "            yield {\n",
        "                'author': quote.xpath('span/small/text()').getall(),\n",
        "                'text': quote.css('span.text::text').getall(),\n",
        "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
        "            }\n",
        "\n",
        "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
        "        if next_page is not None:\n",
        "            yield response.follow(next_page, self.parse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3BGaXI7e-TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a295720d-258e-403c-da3b-8c4ef1bb70fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-10 14:32:04 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: scrapybot)\n",
            "2022-11-10 14:32:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 154, in execute\n",
            "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 109, in _run_print_help\n",
            "    func(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 162, in _run_command\n",
            "    cmd.run(args, opts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/commands/runspider.py\", line 47, in run\n",
            "    module = _import_file(filename)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/commands/runspider.py\", line 19, in _import_file\n",
            "    module = import_module(fname)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/QuotesSpider.py\", line 8\n",
            "    ''https://quotes.toscrape.com/page/1/',\n",
            "          ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!scrapy runspider /content/QuotesSpider.py -o quotes.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZNtHL49gQ6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77ab518-fb57-49e6-8963-d32e435924fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jobs_spider.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile jobs_spider.py \n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class JobsSpider(scrapy.Spider):\n",
        "    name = \"jobs\"\n",
        "    start_urls = [\n",
        "        'https://www.offre-emploi.tn/',\n",
        "        \n",
        "    ]\n",
        "    def parse(self, response):\n",
        "      for js in response.css('article.js_result_row'):\n",
        "        yield {\n",
        "                'title': js.css('.jobTitle span::text').getall(),\n",
        "                'location': js.css('.location a:nth-child(even) span::text').getall(),\n",
        "                'poste': js.css('.location a:nth-child(odd) span::text').getall(),\n",
        "                'preview': js.css('.preview::text').getall(),\n",
        "                'date':js.css(' time::text').getall(),\n",
        "              }\n",
        "      next_page = response.css('.pagingWrapper ul li:last-child a::attr(href)').get()\n",
        "      if next_page is not None:\n",
        "        yield response.follow(next_page, callback=self.parse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMec4WbFgwL6"
      },
      "outputs": [],
      "source": [
        "!scrapy runspider /content/jobs_spider.py -o jobs.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chUyKDBKC7HT"
      },
      "source": [
        "**                    LE 2EME SITE\n",
        " WEB                          **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCYvw5YrjmGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a0d0a8-e999-4877-eb9f-cd2b7a932eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jobs_spider2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile jobs_spider2.py \n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class JobsSpider2(scrapy.Spider):\n",
        "    name = \"jobs2\"\n",
        "    start_urls = [\n",
        "        'https://www.tanitjobs.com/jobs/',\n",
        "        \n",
        "    ]\n",
        "    def parse(self, response):\n",
        "      for media in response.css('article.media'):\n",
        "\n",
        "        yield {\n",
        "                'title':media.css('a.link::text').get(),\n",
        "\n",
        "              }\n",
        "      next_page = response.css('a.bx.next a::attr(href)').get()\n",
        "      if next_page is not None:\n",
        "        yield response.follow(next_page, callback=self.parse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEpJWH3OmwLE",
        "outputId": "4eb5ff77-2373-42fa-e936-5055d4e323db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-10 14:30:01 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: scrapybot)\n",
            "2022-11-10 14:30:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-10 14:30:01 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'SPIDER_LOADER_WARN_ONLY': True}\n",
            "/usr/local/lib/python3.7/dist-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "2022-11-10 14:30:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-11-10 14:30:01 [scrapy.extensions.telnet] INFO: Telnet Password: 5a6f79fcee93098c\n",
            "2022-11-10 14:30:01 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-10 14:30:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-10 14:30:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-10 14:30:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-10 14:30:01 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-10 14:30:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-10 14:30:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-10 14:30:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.tanitjobs.com/jobs/> (failed 1 times): 503 Service Unavailable\n",
            "2022-11-10 14:30:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.tanitjobs.com/jobs/> (failed 2 times): 503 Service Unavailable\n",
            "2022-11-10 14:30:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.tanitjobs.com/jobs/> (failed 3 times): 503 Service Unavailable\n",
            "2022-11-10 14:30:01 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.tanitjobs.com/jobs/> (referer: None)\n",
            "2022-11-10 14:30:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.tanitjobs.com/jobs/>: HTTP status code is not handled or not allowed\n",
            "2022-11-10 14:30:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-10 14:30:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 666,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 26902,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/503': 3,\n",
            " 'elapsed_time_seconds': 0.371512,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 10, 14, 30, 1, 908403),\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/503': 1,\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 98877440,\n",
            " 'memusage/startup': 98877440,\n",
            " 'response_received_count': 1,\n",
            " 'retry/count': 2,\n",
            " 'retry/max_reached': 1,\n",
            " 'retry/reason_count/503 Service Unavailable': 2,\n",
            " 'scheduler/dequeued': 3,\n",
            " 'scheduler/dequeued/memory': 3,\n",
            " 'scheduler/enqueued': 3,\n",
            " 'scheduler/enqueued/memory': 3,\n",
            " 'start_time': datetime.datetime(2022, 11, 10, 14, 30, 1, 536891)}\n",
            "2022-11-10 14:30:01 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy runspider /content/jobs_spider2.py -o jobs2.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMfvasG-0Y8z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIHAKxmOCCOK"
      },
      "source": [
        "*le 3eme SITE web *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YPo1lK8CTjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "cf9ab7c1-ae43-4168-90f8-bfe4dc5feb60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-d51d85125443>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    data=pd.read_json('/content/jobs_spider.py'), orient='records')\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.read_json('/content/jobs_spider.py'), orient='records')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}